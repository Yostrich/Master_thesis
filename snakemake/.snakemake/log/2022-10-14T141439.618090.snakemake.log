Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Conda environments: ignored
Job stats:
job                      count    min threads    max threads
---------------------  -------  -------------  -------------
NegCoverageDropToBed         1              1              1
NegEffRatios                 1              1              1
PosCoverageDropToBed         1              1              1
addTotalReadsToCounts        1              1              1
all                          1              1              1
clusterReads                 1              1              1
combineCovAndPeaks           1              1              1
createGenomecov              1              1              1
extractSequencesTTS          2              1              1
termseqPeakCalling           1              1              1
total                       11              1              1

Select jobs to execute...

[Fri Oct 14 14:14:40 2022]
rule PosCoverageDropToBed:
    input: TTS_LUZ7_t5/eff_ratios_LUZ7_t5.plus.drop.coverage
    output: TTS_LUZ7_t5/TSS_LUZ7_t5.plus.bed
    jobid: 54
    reason: Missing output files: TTS_LUZ7_t5/TSS_LUZ7_t5.plus.bed
    wildcards: phage=LUZ7, ident=t5
    resources: tmpdir=/tmp

RuleException in rule PosCoverageDropToBed in line 76 of /mnt/c/Users/danis/Desktop/Thesis/pipeline_snakemake/workflow/rules/annotateTTS.smk:
NameError: The name 'print $1, $2 - up, $2 + down, "TTS_" NR, "+"' is unknown in this context. Please make sure that you defined that variable. Also note that braces not used for variable access have to be escaped by repeating them, i.e. {{print $1}}, when formatting the following:

        awk -v up={params.up} -v down={params.down} -v FS='	' -v OFS='	' -F ' ' '{print $1, $2 - up, $2 + down, "TTS_" NR, "+"}' {input} | uniq > {output}
        sed -i 's/"//g' {output}
        
